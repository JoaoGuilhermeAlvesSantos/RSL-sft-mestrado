@inproceedings{9474713,
  author    = {Schneider, Elisa Terumi Rubel and de Souza, João Vitor Andrioli and Gumiel, Yohan Bonescki and Moro, Claudia and Paraiso, Emerson Cabrera},
  booktitle = {2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)},
  title     = {A GPT-2 Language Model for Biomedical Texts in Portuguese},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {474-479},
  abstract  = {Electronic health records (EHRs) contain patient-related information formed by structured and unstructured data, a valuable data source for Natural Language Processing (NLP) in the healthcare domain. The contextual word embeddings and Transformer-based models have proved their potential, reaching state-of-the-art for various NLP tasks. Although the performance for downstream NLP tasks with free-texts written in English has recently improved, less resource is available considering clinical texts and low-resource languages such as Portuguese. Our objective is to develop a Generative Pre-trained Transformer 2 (GPT-2) language model for Portuguese to support clinical and biomedical NLP tasks. We fine-tuned a generic Portuguese GPT-2 model to corpora of biomedical texts written in Portuguese, using transfer learning. We experimented on a public dataset, manually annotated for detecting patient fall, i.e., a classification task. Our in-domain GPT-2 model outperformed the generic Portuguese GPT-2 model by 3.43 in F1-score (weighted). Our preliminary results show that transfer learning with domain literature can benefit Portuguese biomedical NLP tasks, aligned with other languages' results.},
  keywords  = {Electric potential;Biological system modeling;Computational modeling;Transfer learning;Medical services;Natural language processing;Task analysis;gpt 2;transformer;nlp;medical domain},
  doi       = {10.1109/CBMS52027.2021.00056},
  issn      = {2372-9198},
  month     = {June}
}@inproceedings{10650878,
  author    = {Da Rocha Junqueira, Júlia and Lopes, Émerson P. and Da S. M., Claudio Luis and Silva, Félix Leonel V. and Carvalho, Eduarda Abreu and Freitas, Larissa and Brisolara, Ulisses},
  booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Sabiá in Action: An Investigation of its Abilities in Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering},
  year      = {2024},
  volume    = {},
  number    = {},
  pages     = {1-8},
  abstract  = {This research investigates the efficacy of the versatile Sabiá-7B model, employed to decipher the complexities of Portuguese language across various tasks. Leveraging state-of-the-art architecture, the model extends LLaMA-7B pre-training to represent the Portuguese language better. This study focuses on evaluating Sabiá-7B’s performance in Aspect-Based Sentiment Analysis (ABSA), Hate Speech Detection (HS), Irony Detection (ID), and Question-Answering (QA) tasks using a few-shot approach. Employing the few-shot method and prompt engineering throughout task executions, our research revealed that Sabiá-7B exhibits notable proficiency, mainly when provided with ample examples during few-shot extraction. However, particular challenges emerged, especially in QA tasks, where the model displayed limitations in generating precise answers compared to expected exact responses. This limitation resulted in the inclusion of extraneous words, potentially classified as irrelevant, impeding the accurate identification of an exact match. Our investigation sheds light on the strengths and potential limitations of Sabiá-7B in various NLP domains. As AI capabilities continue to advance, understanding these intricacies becomes essential for practical applications and the field’s ongoing development.},
  keywords  = {Sentiment analysis;Accuracy;Hate speech;Neural networks;Complexity theory;Prompt engineering;Artificial intelligence},
  doi       = {10.1109/IJCNN60899.2024.10650878},
  issn      = {2161-4407},
  month     = {June}
}@inproceedings{11078353,
  author    = {Santos, Daniel and Nogueira, Vitor Beires and Quaresma, Paulo},
  booktitle = {2025 4th International Conference on Computer Technologies (ICCTech)},
  title     = {Parameter Efficient Fine-Tunning of LLMs: Application to Machine Translation from English to Portuguese},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {24-28},
  abstract  = {Fine-tuning Large Language Models (LLMs) for specific tasks, such as machine translation, is a computationally expensive process that often requires substantial hardware resources. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) and Quantized Low-Rank Adaptation (QLoRA), offer a resource-efficient alternative by significantly reducing the number of trainable parameters and memory requirements. In this work, we compare the performance and memory efficiency of LoRA and QLoRA on English-Portuguese translation tasks, utilizing two cutting edge LLMs, Meta LLaMA 3.1 8B and Mistral 7B. Our experiments demonstrate that both LoRA and QLoRA achieve substantial memory savings. Moreover, this work underscores the practical advantages of LoRA and QLoRA in resource-constrained environments, providing a foundation for further optimization and experimentation in machine translation using large language models.},
  keywords  = {Translation;Large language models;Memory management;Hardware;Machine translation;Optimization;LLM;fine-tunning;LoRA;QLoRA;Machine Translation},
  doi       = {10.1109/ICCTech66294.2025.00014},
  issn      = {},
  month     = {Feb}
}@inproceedings{11124084,
  author    = {Amorim, Annie and Assis, Gabriel and de Oliveira, Daniel and Paes, Aline},
  booktitle = {2025 28th International Conference on Information Fusion (FUSION)},
  title     = {Exploring Language Model Fusion to Improve Generalization in Portuguese Hate Speech Detection},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {1-8},
  abstract  = {As the number of fine-tuned language models for specialized domains and tasks continues to grow, managing a diverse set of solutions presents increasing challenges regarding scalability and adaptability. In this context, model-fusion techniques offer a promising approach to enhancing generalization by leveraging knowledge from multiple trained models. This paper investigates the fusion of pre-trained BERT-based models using ten different fusion strategies: (i) Simple Merging, (ii) Select Simple Merging, (iii) Fisher Merging, (iv) Select Fisher Merging, (v) RegMean, (vi) Task Arithmetic, (vii) DARE-simple Merging, (viii) TIES-MERGING, (ix) Robust Fine-tuning, and (x) Select Epoch Merging. To assess the effectiveness of these techniques, we conducted experiments in the challenging task of detecting hate speech in Portuguese. Such a task benefits from combining knowledge with model fusion, given that individual models might overlook the context-dependent nature of offensive language and nuanced forms of hate speech. The results using six datasets indicate that TIES-MERGING, in particular, can outperform individual models by successfully integrating specialized knowledge into a single solution, a more efficient and robust model.},
  keywords  = {Adaptation models;Analytical models;Social networking (online);Reviews;Computational modeling;Scalability;Merging;Hate speech;Context modeling;Arithmetic;model fusion;generalization;BERT;hate speech;Portuguese},
  doi       = {10.23919/FUSION65864.2025.11124084},
  issn      = {},
  month     = {July}
}@inproceedings{10394189,
  author    = {Noguti, Mariana and Vellasques, Eduardo and Oliveira, Luiz S.},
  booktitle = {2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  title     = {A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1840-1845},
  abstract  = {Recent advances in language modelling has significantly decreased the need of labelled data in text classification tasks. Transformer-based models, pre-trained on unlabeled data, can outmatch the performance of models trained from scratch for each task. However, the amount of labelled data need to fine-tune such type of model is still considerably high for domains requiring expert-level annotators, like the legal domain. This paper investigates the best strategies for optimizing the use of a small labeled dataset and large amounts of unlabeled data and perform a classification task in the legal area with 50 predefined topics. More specifically, we use the records of demands to a Brazilian Public Prosecutor's Office aiming to assign the descriptions in one of the subjects, which currently demands deep legal knowledge for manual filling. The task of optimizing the performance of classifiers in this scenario is especially challenging, given the low amount of resources available regarding the Portuguese language, especially in the legal domain. Our results demonstrate that classic supervised models such as logistic regression and SVM and the ensembles random forest and gradient boosting achieve better performance along with embeddings extracted with word2vec when compared to BERT language model. The latter demonstrates superior performance in association with the architecture of the model itself as a classifier, having surpassed all previous models in that regard. The best result was obtained with Unsupervised Data Augmentation (UDA), which jointly uses BERT, data augmentation, and strategies of semi-supervised learning, with an accuracy of 80.7% in the aforementioned task.},
  keywords  = {Support vector machines;Law;Text categorization;Data augmentation;Transformers;Data models;Task analysis},
  doi       = {10.1109/SMC53992.2023.10394189},
  issn      = {2577-1655},
  month     = {Oct}
}@article{11208574,
  author   = {Pajón-Sanmartín, Alejandro and De Arriba-Pérez, Francisco and García-Méndez, Silvia and Leal, Fátima and Malheiro, Benedita and Carlos Burguillo-Rial, Juan},
  journal  = {IEEE Access},
  title    = {Unraveling Emotions With Pre-Trained Models},
  year     = {2025},
  volume   = {13},
  number   = {},
  pages    = {182458-182473},
  abstract = {Transformer models have significantly advanced the field of emotion recognition. However, there are still open challenges when exploring open-ended queries for Large Language Models (llms). Although current models offer good results, automatic emotion analysis in open texts presents significant challenges, such as contextual ambiguity, linguistic variability, and difficulty interpreting complex emotional expressions. These limitations make the direct application of generalist models difficult. Accordingly, this work compares the effectiveness of fine-tuning and prompt engineering in emotion detection in three distinct scenarios: (i) performance of fine-tuned pre-trained models and general-purpose llms using simple prompts; (ii) effectiveness of different emotion prompt designs with llms; and (iii) impact of emotion grouping techniques on these models. Experimental tests attain metrics above 70% with a fine-tuned pre-trained model for emotion recognition. Moreover, the findings highlight that llms require structured prompt engineering and emotion grouping to enhance their performance. These advancements improve sentiment analysis, human-computer interaction, and understanding of user behavior across various domains.},
  keywords = {Emotion recognition;Transformers;Analytical models;Data models;Prompt engineering;Biological system modeling;Accuracy;Context modeling;Sentiment analysis;Linguistics;Emotion recognition;large language models;natural language processing;open-ended responses;prompt engineering;transformer models},
  doi      = {10.1109/ACCESS.2025.3623877},
  issn     = {2169-3536},
  month    = {}
}@article{11127074,
  author   = {Raychawdhary, Nilanjana and Bhattacharya, Sutanu and Seals, Cheryl and Dozier, Gerry V.},
  journal  = {IEEE Access},
  title    = {Empowering Sentiment Analysis in African Low-Resource Languages Through Transformer Models and Strategic Language Selection},
  year     = {2025},
  volume   = {13},
  number   = {},
  pages    = {147859-147873},
  abstract = {This research addresses the significant challenges of sentiment analysis in low-resource African languages by utilizing advanced transformer-based models to bridge gaps in natural language processing (NLP) for underrepresented linguistic communities. Covering 12 diverse languages: Hausa, Yoruba, Igbo, Nigerian Pidgin, Amharic, Algerian Arabic, Moroccan Arabic Darija, Swahili, Kinyarwanda, Twi, Mozambican Portuguese, and Xitsonga. Our work includes pre-trained language models (PLMs) such as XLM-R, AfroXLMR, AfriBERTa, and mDeBERTaV3, fine-tuning them for the specific task of sentiment classification. In particular, using datasets from the AfriSenti SemEval 2023 Shared Task 12, this process involves tailoring multilingual transformer models to low-resource African languages, enabling them to capture complex linguistic nuances and sentiment polarities effectively. The results demonstrate significant improvements across key metrics, including accuracy, precision, recall, and weighted F1 score. Notably, our work using AfroXLMR achieves the top 1 rank, with a weighted F1 score of 75.8%, outperforming all other submissions. In addition to multilingual transformer models, we evaluated traditional machine learning models, including CNN, Naïve Bayes, and SVM, to compare their performance. The evaluation focused on zero-shot sentiment classification for Tigrinya, a low-resource African language not seen during model training. The results indicate that while traditional models produced meaningful results, multilingual transformer models achieved higher performance in the zero-shot setting. This highlights both the challenges and potential of multilingual transfer approaches for sentiment analysis in African low-resource languages. Moreover, this research highlights the transformative role of Artificial Intelligence (AI) in addressing linguistic diversity, fostering digital inclusion, and enabling reliable sentiment analysis. Furthermore, it lays the groundwork for future advancements in processing low-resource languages and emphasizes the importance of promoting cultural inclusivity through AI technologies.},
  keywords = {Sentiment analysis;Multilingual;Transformers;Natural language processing;Adaptation models;Hidden Markov models;Market research;Cultural differences;Syntactics;Social networking (online);Adaptive pretraining;African low-resource languages;cross-lingual transfer learning;multilingual sentiment classification;natural language processing (NLP);transformer models},
  doi      = {10.1109/ACCESS.2025.3599480},
  issn     = {2169-3536},
  month    = {}
}@inproceedings{10377080,
  author    = {Kim, Minsu and Yeo, Jeong Hun and Choi, Jeongsoo and Ro, Yong Man},
  booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {15313-15325},
  abstract  = {This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.},
  keywords  = {Visualization;Computer vision;Lips;Computational modeling;Transforms;Predictive models;Data models},
  doi       = {10.1109/ICCV51070.2023.01409},
  issn      = {2380-7504},
  month     = {Oct}
}@article{10330588,
  author   = {Riedel, Pascal and Reichert, Manfred and Von Schwerin, Reinhold and Hafner, Alexander and Schaudt, Daniel and Singh, Gaurav},
  journal  = {IEEE Access},
  title    = {Performance Analysis of Federated Learning Algorithms for Multilingual Protest News Detection Using Pre-Trained DistilBERT and BERT},
  year     = {2023},
  volume   = {11},
  number   = {},
  pages    = {134009-134022},
  abstract = {Data scientists in the Natural Language Processing (NLP) field confront the challenge of reconciling the necessity for data-centric analyses with the imperative to safeguard sensitive information, all while managing the substantial costs linked to the collection process of training data. In a Federated Learning (FL) system, these challenges can be alleviated by the training of a global model, eliminating the need to centralize sensitive data of clients. However, distributed NLP data is usually Non-Independent and Identically Distributed (Non-IID), which leads to poorer generalizability of the global model when trained with Federated Averaging (FedAvg). Recently proposed extensions to FedAvg promise to improve the global model performance on Non-IID data. Yet, such advanced FL algorithms trained on multilingual Non-IID texts have not been studied in industry and academia in detail. This paper compares, for the first time, the FL algorithms: FedAvg, FedAvgM, FedYogi, FedAdam and FedAdagrad for a binary text classification task using 12078 tailored real-world news reports in English, Portuguese, Spanish and Hindi. For this objective, pre-trained DistilBERT and BERT models fine-tuned with these texts are used. The paper results show that FedYogi is the most stable and robust FL algorithm when DistilBERT is used, achieving an average macro F1 score of 0.7789 for IID and 0.7755 for Non-IID protest news. The study also exhibits that BERT models trained with weighted FedAvg and FedAvgM can achieve a similar prediction power as centralized language models, demonstrating the potential of leveraging FL in the NLP domain without the need to collect data centrally.},
  keywords = {Data models;Transformers;Analytical models;Training;Distributed databases;Natural language processing;Computational modeling;Privacy;natural language processing;distributed learning;optimization;federated algorithms;data distributions},
  doi      = {10.1109/ACCESS.2023.3334910},
  issn     = {2169-3536},
  month    = {}
}
