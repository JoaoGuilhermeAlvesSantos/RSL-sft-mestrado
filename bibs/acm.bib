@inproceedings{10.1145/3626772.3657819,
author = {Guo, Ping and Ren, Yubing and Hu, Yue and Cao, Yanan and Li, Yunpeng and Huang, Heyan},
title = {Steering Large Language Models for Cross-lingual Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657819},
doi = {10.1145/3626772.3657819},
abstract = {In today's digital age, accessing information across language barriers poses a significant challenge, with conventional search systems often struggling to interpret and retrieve multilingual content accurately. Addressing this issue, our study introduces a novel integration of applying Large Language Models (LLMs) as Cross-lingual Readers in information retrieval systems, specifically targeting the complexities of cross-lingual information retrieval (CLIR). We present an innovative approach: Activation Steered Multilingual Retrieval (ASMR) that employs "steering activations''-a method to adjust and direct the LLM's focus-enhancing its ability to understand user queries and generate accurate, language-coherent responses. ASMR adeptly combines a Multilingual Dense Passage Retrieval (mDPR) system with an LLM, overcoming the limitations of traditional search engines in handling diverse linguistic inputs. This approach is particularly effective in managing the nuances and intricacies inherent in various languages. Rigorous testing on established benchmarks such as XOR-TyDi QA, and MKQA demonstrates that ASMR not only meets but surpasses existing standards in CLIR, achieving state-of-the-art performance. The results of our research hold significant implications for understanding the inherent features of how LLMs understand and generate natural languages, offering an attempt towards more inclusive, effective, and linguistically diverse information access on a global scale.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {585–596},
numpages = {12},
keywords = {activation steering, cross-lingual information retrieval, large language models},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inbook{10.5555/3716662.3716755,
author = {Nunes, Jos\'{e} Luiz and Almeida, Guilherme F. C. F. and de Araujo, Marcelo and Barbosa, Simone D. J.},
title = {Are Large Language Models Moral Hypocrites? A Study Based on Moral Foundations},
year = {2025},
publisher = {AAAI Press},
abstract = {Large language models (LLMs) have taken centre stage in debates on Artificial Intelligence. Yet there remains a gap in how to assess LLMs' conformity to important human values. In this paper, we investigate whether state-of-the-art LLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid results) are moral hypocrites. We employ two research instruments based on the Moral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which investigates which values are considered morally relevant in abstract moral judgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate moral cognition in concrete scenarios related to each moral foundation. We characterise conflicts in values between these different abstractions of moral evaluation as hypocrisy. We found that both models displayed reasonable consistency within each instrument compared to humans, but they displayed contradictory and hypocritical behaviour when we compared the abstract values present in the MFQ to the evaluation of concrete moral violations of the MFV.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1074–1087},
numpages = {14}
}

@inproceedings{10.1145/3589334.3645701,
author = {Guo, Ping and Hu, Yue and Cao, Yanan and Ren, Yubing and Li, Yunpeng and Huang, Heyan},
title = {Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645701},
doi = {10.1145/3589334.3645701},
abstract = {In the contemporary digital landscape, search engines play an invaluable role in information access, yet they often face challenges in Cross-Lingual Information Retrieval (CLIR). Though attempts are made to improve CLIR, current methods still leave users grappling with issues such as misplaced named entities and lost cultural context when querying in non-native languages. While some advances have been made using Neural Machine Translation models and cross-lingual representation, these are not without limitations. Enter the paradigm shift brought about by Large Language Models (LLMs), which have transformed search engines from simple retrievers to generators of contextually relevant information. This paper introduces the Multilingual Information Model for Intelligent Retrieval (MIMIR). Built on the power of LLMs, MIMIR directly responds in the language of the user's query, reducing the need for post-search translations. Our model's architecture encompasses a dual-module system: a retriever for searching multilingual documents and a responder for crafting answers in the user's desired language. Through a unique unified training framework, with the retriever serving as a reward model supervising the responder, and in turn, the responder producing synthetic data to refine the retriever's proficiency, MIMIR's retriever and responder iteratively enhance each other. Performance evaluations via CLEF and MKQA benchmarks reveal MIMIR's superiority over existing models, effectively addressing traditional CLIR challenges.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {1529–1538},
numpages = {10},
keywords = {cross-lingual information retrieval, large language models, search generative experience},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3709026.3709118,
author = {Liu, Weizhuo and Gao, Zhe},
title = {Improving Automated Audio Captioning with LLM Decoder and BEATs Audio Encoder},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709118},
doi = {10.1145/3709026.3709118},
abstract = {Automated Audio Captioning (AAC) focuses on producing detailed descriptions for a wide range of sounds, including those from nature and human activities. Recently, this field has garnered significant research attention, with the latest advancements in AAC systems largely driven by the use of sequence-to-sequence (seq2seq) frameworks. These frameworks are increasingly powered by powerful models like Transformers, which have become the backbone of state-of-the-art AAC approaches. We propose an advanced Automated Audio Captioning (AAC) system that leverages the LLaMA3.1 decoder for robust language generation and the BEATs encoder for fine-grained acoustic feature extraction. These components are connected via a Q-Former intermediate layer, ensuring seamless audio-to-text alignment. To overcome the challenge of limited training data, we employ a sophisticated data augmentation strategy using Qwen&nbsp;, enhancing the model’s generalization and resilience. A multi-step training process with low-rank adaptation (LoRA) further optimizes performance. Our method obtains a 28.5 SPIDEr-FL score, outperforming the participates of DCASE 2023 Task 6A.Our code will be stored on GitHub1.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {252–259},
numpages = {8},
keywords = {automated audio captioning, audio encoding, large language model},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3711896.3737271,
author = {Yang, Zachary and Tullo, Domenico and Rabbany, Reihaneh},
title = {Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737271},
doi = {10.1145/3711896.3737271},
abstract = {Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96\% to 58.88\%, with particularly strong performance in German, surpassing the English benchmark of 45.39\%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5161–5170},
numpages = {10},
keywords = {LLM-assisted label transfer, chat moderation, scaling, soft-prompting, toxicity},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3685650.3685664,
author = {Oliveira, Hil\'{a}rio and Lins, Rafael Dueire},
title = {Assessing Abstractive and Extractive Methods for Automatic News Summarization},
year = {2024},
isbn = {9798400711695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685650.3685664},
doi = {10.1145/3685650.3685664},
abstract = {Automatic Text Summarization (ATS) is a research area that originated in the late 1950s and has gained increasing importance with the surge of text data available today. ATS approaches are generally classified into extractive and abstractive methods. Extractive summarization selects the most relevant sentences from a text and copies them verbatim into the summary. Abstractive text summarization aims to generate a more cohesive and concise version of the original text. Recently, pre-trained and large language models such as BERT, BART, Pegasus, Llama, Gemma, and GPT-3 have revolutionized numerous natural language tasks, including creating more humanlike summaries. Despite the progress in recent years, there is room for improvement in directly comparing different summarization tools using a corpus containing high-quality reference summaries. This paper evaluates the quality of summaries generated by such tools, comparing the results obtained from abstractive and extractive summarization methods. Experiments were conducted using the CNN-corpus, focusing on news articles written in English and employing four evaluation measures. The experimental results unveiled that the Pegasus model outperformed others in abstractive summarization. These findings highlight the potential for further advancements in the field, suggesting that using more specialized models explicitly tailored for the summarization task may yield superior results compared to large general-purpose language models.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2024},
articleno = {12},
numpages = {10},
keywords = {BART, CNN-corpus, Llama, Mistral and GPT-3, Pegasus, Text summarization, abstractive summarization, extractive summarization},
location = {San Jose, CA, USA},
series = {DocEng '24}
}

@inproceedings{10.1145/3726302.3731934,
author = {Dinzinger, Michael and Caspari, Laura and Ghosh Dastidar, Kanishka and Mitrovi\'{c}, Jelena and Granitzer, Michael},
title = {WebFAQ: A Multilingual Collection of Natural Q&amp;A Datasets for Dense Retrieval},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3731934},
doi = {10.1145/3726302.3731934},
abstract = {We present WebFAQ, a large-scale collection of open-domain question answering datasets derived from FAQ-style schema.org annotations. In total, the data collection consists of 96 million natural question-answer (QA) pairs across 75 languages, including 47 million (49\%) non-English samples. WebFAQ further serves as the foundation for 49 monolingual retrieval benchmarks with a total size of 11.2 million QA pairs (5.9 million non-English). These datasets are carefully curated through refined filtering and near-duplicate detection, yielding high-quality resources for training and evaluating multilingual dense retrieval models. To empirically confirm WebFAQ's efficacy, we use the collected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through this process of dataset-specific fine-tuning, the model achieves significant retrieval performance gains, which generalize - beyond WebFAQ - to other multilingual retrieval benchmarks evaluated in zero-shot setting. Last but not least, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora spanning over 1000 language pairs using state-of-the-art bitext mining and automated LLM-assessed translation evaluation. Due to our advanced, automated method of bitext dataset generation, the resulting bilingual corpora demonstrate higher translation quality compared to similar datasets. WebFAQ and all associated resources are publicly available on GitHub and HuggingFace.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3802–3811},
numpages = {10},
keywords = {cross-lingual information retrieval, dense retrieval, multilingual text embedding, question answering},
location = {Padua, Italy},
series = {SIGIR '25}
}

