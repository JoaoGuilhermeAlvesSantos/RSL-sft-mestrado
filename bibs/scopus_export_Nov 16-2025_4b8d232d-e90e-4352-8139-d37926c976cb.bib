Scopus
EXPORT DATE: 16 November 2025

@ARTICLE{de Souza2025973,
	author = {de Souza, José Victor and Amamou, Hazem and Chen, Rubing and Salari, Elmira and Gubelmann, Reto and Niklaus, Christina and Serpa, Talita De and Lima, Marcela Marques De Freitas and Pinto, Paula Tavares and Kshirsagar, Shruti Rajendra and Davoust, Alan and Handschuh, Siegfried and Avila, Anderson Raymundo},
	title = {Cross-Lingual Keyword Extraction for Pesticide Terminology in Brazilian Portuguese and English},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {973 - 990},
	doi = {10.5753/jbcs.2025.5815},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019700300&doi=10.5753%2Fjbcs.2025.5815&partnerID=40&md5=85ee75baf4550666a307310cd04d1c83},
	abstract = {Agriculture plays a crucial role in Brazil’s economy. As the country intensifies its activities in the sector, the use of pesticides also increases. Hence, the risks associated with pesticide-laden food consumption have become a concern for chemistry researchers. An issue affecting regulatory standardization of pesticides in Brazil is the difficulty in translating pesticide names, particularly from English. For example, the word malathion can be translated from English to Portuguese as malatiom or malatião, resulting in inconsistent labeling. This issue extends to the broader problem of translating highly technical terms between languages, in particular for low-resource languages. In this work, we investigate terminological variation in the chemistry of organophosphorus pesticides. Our goal is to study strategies for domain-specific multilingual keyword extraction. To that end, two corpora were built based on pesticide-related scientific documents in Brazilian Portuguese and English, which led to a total of 84 and 210 texts, respectively, representing the low-and high-resource languages in this study. We then assessed 6 methods for keyword extraction: Simple Maths, TF-IDF, YAKE, TextRank, MultipartiteRank, and KeyBERT. We relied on a multilingual contextual BERT embedding to retrieve corresponding pesticide names in the target language. Finetuning was also explored to improve the multilingual representation further. Moreover, we evaluated the use of large language models (LLMs) combined with the recent retrieval-augmented generation (RAG) framework. As a result, we found that the contextual approach, combined with fine-tuning, provided the best results, contributing to enhancing Pesticide Terminology Extraction in a multilingual scenario. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {BERT embeddings; Multilingual extraction; pesticides; word alignment},
	keywords = {Agriculture; Extraction; Terminology; BERT embedding; Cross-lingual; Embeddings; Food consumption; Keywords extraction; Labelings; Low resource languages; Multilingual extraction; Technical terms; Word alignment; Pesticides},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Schuck2025885,
	author = {Schuck, André Da Fonseca and García, Gabriel Lino and Manesco, João Renato Ribeiro and Paiola, Pedro Henrique and Papa, João Paulo},
	title = {Evaluating Large Language Models for Brazilian Portuguese Sentiment Analysis: A Comparative Study of Multilingual State-of-the-Art vs. Brazilian Portuguese Fine-Tuned LLMs},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {885 - 917},
	doi = {10.5753/jbcs.2025.5793},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019512860&doi=10.5753%2Fjbcs.2025.5793&partnerID=40&md5=4cc5c631332491d8296324f529631339},
	abstract = {This study presents an extensive comparative analysis of Large Language Models (LLMs) for sentiment analysis in Brazilian Portuguese texts. We evaluated 23 LLMs—comprising 13 state-of-the-art multilingual models and 10 models specifically fine-tuned for Portuguese—across 12 public annotated datasets from diverse domains, employing the in-context learning paradigm. Our findings demonstrate that large-scale models such as Claude-3.5Sonnet, GPT-4o, DeepSeek-V3, and Sabiá-3 delivered superior results with accuracies exceeding 92%, while smaller models (7-13B parameters) also showed compelling performance with top performers achieving accuracies above 90%. Notably, linguistic specialization through fine-tuning demonstrated mixed results—significantly reducing hallucination rates for some models but not consistently yielding performance improvements across all model types. We also observed that newer model generations frequently outperformed their predecessors, and in the one dataset where traditional machine learning methods were employed by the original authors for sentiment classification, all evaluated LLMs substantially surpassed these traditional approaches. Moreover, smaller-scale models exhibited a tendency toward overgeneration despite explicit instructions. These findings contribute valuable insights to the discourse on language-specific model optimization and establish empirical benchmarks for both multilingual and Portuguese-specialized LLMs in sentiment analysis tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Brazilian Portuguese; Comparative Evaluation; In-context Learning; Large Language Models; Model Fine-tuning; Natural Language Processing; Sentiment Analysis},
	keywords = {Classification (of information); Computational linguistics; Learning algorithms; Learning systems; Tuning; Brazilian portuguese; Comparative evaluations; Context learning; Fine tuning; In contexts; In-context learning; Language model; Language processing; Large language model; Model fine-tuning; Natural language processing; Natural languages; Sentiment analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Barbosa2025858,
	author = {Barbosa, André and Silveira, Igor Cataneo and Mauá, Denis Deratani Deratani},
	title = {An Empirical Analysis of Large Language Models for Automated Cross-Prompt Essay Trait Scoring in Brazilian Portuguese},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {858 - 871},
	doi = {10.5753/jbcs.2025.5817},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019318486&doi=10.5753%2Fjbcs.2025.5817&partnerID=40&md5=ff3e79604008fc7f7b9414a31a6a11e1},
	abstract = {The development of automated essay grading systems with minimal human intervention has been pursued for decades. While these systems have advanced significantly in English, there is still a lack of in-depth analysis of the use of modern Large Language Models for automatic essay scoring in Portuguese. This work addresses this gap by evaluating different language model architectures (encoder-only, decoder-only, reasoning-based), fine-tuning and prompt engineering strategies. Our study focuses on scoring argumentative essays written as practice exercises for the Brazilian national entrance exam regarding five trait-specific criteria. Our results show that no architecture is always dominant, and that encoder-only models offer a good balance between accuracy and computational cost. We obtain state-of-the-art results for the dataset, obtaining trait-specific performance that ranges from .60 to .73 measured in Quadratic Weighted Kappa. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Essay Scoring; Large Language Models; Natural Language Processing},
	keywords = {Computational linguistics; Automated essay grading; Automatic essay scoring; Empirical analysis; Grading system; Human intervention; Language model; Language processing; Large language model; Natural language processing; Natural languages; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Paiola2025918,
	author = {Paiola, Pedro Henrique and García, Gabriel Lino and Correia, João Vitor Mariano and Manesco, João Renato Ribeiro and Garcia, Ana Lara Alves and Papa, João Paulo},
	title = {The Bode Family of Large Language Models: Investigating the Frontiers of LLMs in Brazilian Portuguese},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {918 - 939},
	doi = {10.5753/jbcs.2025.5812},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019318059&doi=10.5753%2Fjbcs.2025.5812&partnerID=40&md5=59f4f6058376951e7378120eb2956ed3},
	abstract = {The rapid advancement of Large Language Models (LLMs) has significantly impacted Natural Language Processing, yet their effectiveness remains uneven across languages. Most state-of-the-art models are trained predominantly in English, leading to performance disparities in lower-resource languages such as Brazilian Portuguese (BP). This paper explores fine-tuning strategies for adapting open-weight LLMs to BP, focusing on dataset translation techniques, linguistic adaptation challenges, and parameter-efficient fine-tuning methods, such as LoRA and Q-LoRA. We present a benchmark analysis evaluating multiple fine-tuning approaches across various open models, establishing a guiding framework for future BP-specific adaptations. Our results showcase the importance of specialized fine-tuning in improving cross-lingual transfer and NLP performance in BP, contributing to the broader goal of enhancing multilingual language model accessibility. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Brazilian Portuguese; Large Language Models; Natural Language Processing; Small Language Models},
	keywords = {Computational linguistics; Tuning; Brazilian portuguese; Fine tuning; Language model; Language processing; Large language model; Natural language processing; Natural languages; Performance; Small language model; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Mucciaccia2025872,
	author = {Mucciaccia, Sérgio Silva and Paixão, Thiago M. and Mutz, Filipe Wall and De Souza, Alberto Ferreira and Badue, Claudine and Oliveira-Santos, Thiago},
	title = {Pt-HotpotQA: Evaluating Multi-Hop Question Answering on Original and Portuguese-translated Datasets Using LLMs},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {872 - 884},
	doi = {10.5753/jbcs.2025.5801},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019316175&doi=10.5753%2Fjbcs.2025.5801&partnerID=40&md5=62c29f71f6810a1f67a117e8d7e0f7cf},
	abstract = {Multi-hop Question Answering (MHQA) advances Natural Language Processing by pushing models to combine information from multiple sources in a series of reasoning steps. Despite substantial advancements in MHQA for English, resources for evaluating Large Language Models (LLMs) in Portuguese remain scarce. To address this gap, we introduce a publicly available Portuguese translation of the HotpotQA dataset, a well-established English MHQA benchmark. We systematically evaluate several variants of the Llama multilingual LLM across both the original and translated datasets, analyzing performance variations by language. Our findings demonstrate that multilingual models consistently perform better in English than in Portuguese, though this gap narrows with increased model size. Additionally, we show the impact of fine-tuning on improving MHQA performance in Portuguese. This study provides valuable insights into optimizing LLMs for multilingual contexts and contributes a relevant benchmark for Portuguese-language MHQA research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cross-lingual Evaluation; Dataset Translation; Large Language Models; Multi-hop QA; Portuguese NLP},
	keywords = {Computational linguistics; Large datasets; Question answering; Translation (languages); Cross-lingual; Cross-lingual evaluation; Dataset translation; Language model; Large language model; Multi-hop QA; Multi-hops; Natural languages; Portuguese NLP; Question Answering; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Ribeiro2025690,
	author = {Ribeiro, Eugénio and Antunes, David and Mamede, Nuno J. and Baptista, Jorge},
	title = {Exploring Few-Shot Approaches to Automatic Text Complexity Assessment in European Portuguese},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {690 - 710},
	doi = {10.5753/jbcs.2025.5820},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014626036&doi=10.5753%2Fjbcs.2025.5820&partnerID=40&md5=1af77bf3f190d784ed246b191428b604},
	abstract = {The automatic assessment of text complexity has an important role to play in the context of language education. In this study, we shift the focus from L2 learners to adult native speakers with low literacy by exploring the new iRead4Skills dataset in European Portuguese. Furthermore, instead of relying on classical machine learning approaches or fine-tuning a pre-trained language model, we leverage the capabilities of prompt-based Large Language Models (LLMs), with a special focus on few-shot prompting approaches. We explore prompts with varying degrees of information, as well as different example selection approaches. Overall, the results of our experiments reveal that even a single example significantly increases the performance of the model and that few-shot approaches generalize better than fine-tuned models. However, automatic complexity assessment is a difficult and highly subjective task that is still far from solved. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Few-Shot Prompting; Large Language Models; Readability; Text Complexity},
	keywords = {Computational complexity; Learning systems; Automatic assessment; Few-shot prompting; Fine tuning; Language education; Language model; Large language model; Low literacies; Machine learning approaches; Readability; Text complexity; Computational linguistics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Lasheras20251005,
	author = {Lasheras, Uriel and Alves, Elioenai and Ponte, Caio and Caminha, Carlos and Pinheiro, Vladia Célia Monteiro},
	title = {Open LLMs Meet Causality in Portuguese: A Corpus-Based Fine-Tuning Approach},
	year = {2025},
	journal = {Journal of the Brazilian Computer Society},
	volume = {31},
	number = {1},
	pages = {1005 - 1030},
	doi = {10.5753/jbcs.2025.5825},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020015231&doi=10.5753%2Fjbcs.2025.5825&partnerID=40&md5=555cb95cb8a2f4140245d13e1598f57e},
	abstract = {Causal reasoning is a key component in the development of more robust, fair, and explainable language models. However, the ability of open-source Large Language Models (LLMs) to perform causal reasoning, especially in languages other than English, remains an open challenge. In this paper, we introduce an expanded version of CaLQuest.PT, a corpus of 2,500 natural questions in Portuguese designed to support multi-level causal evaluation. This dataset enables three layers of classification: (1) causal vs. non-causal questions, (2) causal action types such as cause-seeking, effect-seeking, and recommendation-seeking, and (3) reasoning types based on Pearl’s Ladder of Causality—associational, interventional, and counterfactual. We also present an enhanced Few-Shot Learning prompting strategy and evaluate the performance of open-source models fine-tuned on this corpus. Our results show that, with targeted training and prompt design, smaller open-source LLMs can approach and even surpass the performance of larger models in several causal classification tasks. This study highlights the viability of corpusbased fine-tuning as a low-resource alternative for enhancing causal reasoning in open LLMs and advancing natural language understanding in Portuguese. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Causal Reasoning; Corpus-Based Fine-Tuning; Open Source LLMs; Portuguese NLP},
	keywords = {Computational linguistics; Natural language processing systems; Open systems; Tuning; Causal reasoning; Corpus-based; Corpus-based fine-tuning; Fine tuning; Language model; Multilevels; Open source large language model; Open-source; Performance; Portuguese NLP; Classification (of information)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Melo2025427,
	author = {Melo, Alan and Cabral, Bruno Souza and Claro, Daniela Barreiro},
	title = {Scaling and Adapting Large Language Models for Portuguese Open Information Extraction: A Comparative Study of Fine-Tuning and LoRA},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15414 LNAI},
	pages = {427 - 441},
	doi = {10.1007/978-3-031-79035-5_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219183792&doi=10.1007%2F978-3-031-79035-5_30&partnerID=40&md5=febd17a75deab0cf2a2afa31c6250d1e},
	abstract = {This paper comprehensively investigates the efficacy of different adaptation techniques for Large Language Models (LLMs) in the context of Open Information Extraction (OpenIE) for Portuguese. We compare Full Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA) across a model with 0.5B parameters. Our study evaluates the impact of model size and adaptation method on OpenIE performance, considering precision, recall, and F1 scores, as well as computational efficiency during training and inference phases. We contribute to a high-performing LLM and novel insights into the trade-offs between model scale, adaptation technique, and cross-lingual transferability in the OpenIE task. Our findings reveal significant performance variations across different configurations, with LoRA demonstrating competitive results. We also analyze the linguistic nuances in the Portuguese OpenIE that pose challenges for models primarily trained on English data. This research advances our understanding of LLM adaptation for specialized NLP tasks and provides practical guidelines for deploying these models in resource-constrained and multilingual scenarios. Our work has implications for the broader cross-lingual open information extraction field and contributes to the ongoing discourse on efficient fine-tuning strategies for large pre-trained models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Extraction; Language Model; OpenIE},
	keywords = {Linguistics; Adaptation techniques; Comparatives studies; Cross-lingual; Fine tuning; Information extraction; Language model; Model Adaptation; Model size; Open information extraction; Scalings; Open Data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Nunes2025127,
	author = {Nunes, Rafael Oleques and Puttlitz, Letícia Maria and Boll, Antonio Oss and Spritzer, Andre Suslik and Freitas, Carla Maria Dal Sasso and Balreira, Dennis Giovani and Tavares, Anderson Rocha},
	title = {An Ensemble of LLMs Finetuned with LoRA for NER in Portuguese Legal Documents},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15412 LNAI},
	pages = {127 - 140},
	doi = {10.1007/978-3-031-79029-4_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219176687&doi=10.1007%2F978-3-031-79029-4_9&partnerID=40&md5=735288a7ca292b20f37c305f35c0dd11},
	abstract = {Given the high computational costs of traditional fine-tuning methods and the goal of improving performance,this study investigate the application of low-rank adaptation (LoRA) for fine-tuning BERT models to Portuguese Legal Named Entity Recognition (NER) and the integration of Large Language Models (LLMs) in an ensemble setup. Focusing on the underrepresented Portuguese language, we aim to examine the reliability of extractions enabled by LoRA models and glean actionable insights from the results of both LoRA and LLMs operating in ensembles. Achieving F1-scores of 88.49% for the LeNER-Br corpus and 81.00% for the UlyssesNER-Br corpus, LoRA models demonstrated competitive performance, approaching state-of-the-art standards. Our research demonstrates that incorporating class definitions and counting votes per class substantially improves LLM ensemble results. Overall, this contribution advances the frontiers of AI-powered legal text mining, proposing small models and initial prompt engineering to low-resource conditions that are scalable for broader representation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; LoRA; Named Entity Recognition},
	keywords = {Error correction; Adaptation models; Computational costs; Fine tuning; Fine-tuning methods; Improving performance; Language model; Large language model; Legal documents; Low-rank adaptation; Named entity recognition; Problem oriented languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{García2025228,
	author = {García, Gabriel Lino and Paiola, Pedro Henrique and Garcia, Eduardo Augusto Santos and Manesco, João Renato Ribeiro and Papa, João Paulo},
	title = {GemBode and PhiBode: Adapting Small Language Models to Brazilian Portuguese},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15368 LNCS},
	pages = {228 - 243},
	doi = {10.1007/978-3-031-76607-7_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210251378&doi=10.1007%2F978-3-031-76607-7_17&partnerID=40&md5=a23f66ca3651f8c8b06eded44a6d6244},
	abstract = {Recent advances in generative capabilities provided by large language models have reshaped technology research and human society’s cognitive abilities, bringing new innovative capacities to artificial intelligence solutions. However, the size of such models has raised several concerns regarding their alignment with hardware-limited resources. This paper presents a comprehensive study on training Portuguese-focused Small Language Models (SLMs). We have developed a unique dataset for training our models and employed full fine-tuning, as well as PEFT approaches for comparative analysis. We used Microsoft’s Phi and Google’s Gemma as base models to create our own, named PhiBode and GemBode. These models range from approximately 1 billion to 7 billion parameters, with a total of ten models developed. Our findings provide valuable insights into the performance and applicability of these models, contributing significantly to the field of Portuguese language processing. This research is a step forward in understanding and improving the performance of SLMs in Portuguese. The comparative analysis of the models provides a clear benchmark for future research in this area. The results demonstrate the effectiveness of our training methods and the potential of our models for various applications. This paper significantly contributes to language model training, particularly for the Portuguese language. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bode; Generative Artificial Intelligence; Natural Language Processing; Portuguese; Small Language Models},
	keywords = {Bode; Comparative analyzes; Generative artificial intelligence; Language model; Language processing; Natural language processing; Natural languages; Performance; Portuguese; Small language model; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Freisinger2025276,
	author = {Freisinger, Steffen and Seeberger, Philipp and Ranzenberger, Thomas and Bocklet, Tobias and Riedhammer, Korbinian},
	title = {Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation},
	year = {2025},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	pages = {276 - 280},
	doi = {10.21437/Interspeech.2025-2792},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020082086&doi=10.21437%2FInterspeech.2025-2792&partnerID=40&md5=41717f70aa9630e2374b8cd4e9c6e322},
	abstract = {Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {hierarchical segmentation; spoken content segmentation; table of contents generation; topic segmentation},
	keywords = {Image segmentation; Speech analysis; Content segmentation; Downstream users; Fine tuning; Hierarchical segmentation; Multilevels; Speech transcripts; Spoken content segmentation; Table of content generation; Table of contents; Topic segmentations; Speech communication},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kuzmin20252829,
	author = {Kuzmin, Igor},
	title = {CLEF 2025 JOKER Track: No Pun Left Behind},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4038},
	pages = {2829 - 2837},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019054173&partnerID=40&md5=a3cc5d8c3c0c9b972fda3bdb1ecfbb63},
	abstract = {Humor processing remains a challenging problem for NLP due to linguistic ambiguity, language-specific nuances, and intricate wordplay. The CLEF JOKER 2025 lab tackles this with two tasks we participated in: humour-aware information retrieval in Portuguese and English (Task 1), and pun translation from English to French (Task 2). For Task 1 we developed a hybrid pipeline combining BM25, dense retrieval with multilingual-e5-small, and a cross-encoder reranker, achieving MAP 0.050 and NDCG@100 0.172 in English, and MAP 0.074 and NDCG@100 0.184 in Portuguese. For Task 2 we fine-tuned Lucie-7B-Instruct and CroissantLLMChat-v0.1 using supervised fine-tuning (SFT) and Adaptive Rejection Preference Optimization (ARPO), obtaining a best BLEU of 42.40 (Lucie + SFT) and demonstrating a modest overlap trade-off (41.32 BLEU) when integrating ARPO, while CroissantLLM variants scored 35.17 and 35.28 BLEU. Our experiments show the baseline IR setup underperforms compared to more advanced systems, while the LLMs-based pun translation achieves best results confirming the promise of their cross-lingual wordplay transfer. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Humor Analysis; Humor Retrieval; Humor Translation; Information Retrieval; LLM; Machine Translation},
	keywords = {Artificial intelligence; Computational linguistics; Computer aided language translation; Human engineering; Information management; Machine translation; Natural language processing systems; Advanced systems; Cross-lingual; Fine tuning; Humor analyze; Humor retrieval; Humor translation; LLM; Machine translations; Preference optimizations; Trade off; Economic and social effects; Information retrieval},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vachharajani20252889,
	author = {Vachharajani, Poojan},
	title = {pjmathematician at the CLEF 2025 JOKER Lab Tasks 1, 2 & 3: A Unified Approach to Humour Retrieval and Translation using the Qwen LLM Family},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4038},
	pages = {2889 - 2897},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019046139&partnerID=40&md5=1da6058132e767974c80e10001aeac87},
	abstract = {This paper details the participation of the pjmathematician team in all three tasks of the JOKER 2025 track: Humour-aware Information Retrieval, Pun Translation, and Onomastic Wordplay Translation. Our approach uniformly leverages the Qwen family of large language models (LLMs), applying distinct strategies tailored to each task’s unique challenges. For Task 1, we implemented a two-stage process involving an LLM-based joke filter and explainer followed by a dense retriever, achieving a MAP@1000 of 0.3501 for English and 0.4221 for Portuguese on the test set. For Task 2, we fine-tuned various Qwen models to translate puns from English to French, with our best model (Qwen2.5-14B) attaining a BLEU score of 0.379. For Task 3, we explored zero-shot prompting for the complex task of translating onomastic wordplay, where our Qwen3-32B model achieved an exact match accuracy of 0.22. These results demonstrate the versatility of modern LLMs in handling nuanced and creative language tasks, highlighting the effectiveness of filtering for humour-aware retrieval, fine-tuning for pun translation, and the challenges in zero-shot onomastic translation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Humor; Information Retrieval; Large Language Models; Machine Translation; Wordplay},
	keywords = {Computational linguistics; Machine translation; Best model; Computational humor; Language model; Large language model; Machine translations; Model-based OPC; Test sets; Two-stage process; Unified approach; Wordplay; Information retrieval},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vineetha20251284,
	author = {Vineetha, Baddepudi Venkata Naga Sri Sai},
	title = {Saivineetha at CheckThat! 2025: Exploring Fine-Tuning and Zero-Shot Approaches for Claim Normalization},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4038},
	pages = {1284 - 1289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019045339&partnerID=40&md5=3c6338b40a9da90e750e9c691da6968a},
	abstract = {This paper presents our participation in the CLEF 2025 CheckThat! Lab’s Task 2 which focuses on claims extraction and normalization. This task aims to decompose social media posts into simpler, comprehensible forms. The task spans across 20 languages - English, Arabic, Bengali, Czech, German, Greek, French, Hindi, Korean, Marathi, Indonesian, Dutch, Punjabi, Polish, Portuguese, Romanian, Spanish, Tamil, Telugu, Thai. Our study focuses on two different languages, Hindi and Telugu. Our approach involves Parameter-Efficient Fine-Tuning (PEFT) on multi-lingual Large Language Model (LLM) for Hindi dataset and zero-shot inferencing for Telugu dataset. Our proposed method is ranked third in Hindi with METEOR score of 0.2996 and fourth in Telugu with METEOR score of 0.3774 in the organizer’s leaderboard. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Claim Normalization; Large Language Model (LLM); Parameter-Efficient Fine-tuning (PEFT); Zero-shot inference},
	keywords = {Information management; Large datasets; Tuning; Bengalis; Claim normalization; Fine tuning; Language model; Large language model; Normalisation; Parameter-efficient fine-tuning; Simple++; Social media; Zero-shot inference; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Salim2025554,
	author = {Salim, Md Shahidul and Fu, Lianne and Ramakrishnan, Arav Adikesh and Kwon, Sunjae and Yao, Zonghai and Yu, Hong},
	title = {Enhancing Multilingual Medical Summarization via Contextual Keyword Augmentation},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4038},
	pages = {554 - 568},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019040120&partnerID=40&md5=209f8890ab00c388d418db88221c01e9},
	abstract = {This paper presents the work of the UMass BioNLP team for the MultiClinSUM multilingual medical text summarization task. We introduce MedCOD, a novel framework that improves multilingual summarization through keyword-based contextual augmentation. MedCOD begins by extracting medical keywords from full clinical texts using the Qwen2.5-14B model. These keywords are translated into five languages—English, Spanish, French, German, and Portuguese—using the NLLB 3.3B model, and validated through back-translation and semantic equivalence checking with Qwen2.5-14B. The resulting multilingual keyword chains are incorporated into prompts as a structured context. We evaluate MedCOD using two open-source large language models, Qwen2.5-14B and Phi-4B, in both zero-shot and fine-tuned settings. We fine-tune the model using parameter-efficient LoRA on the MultiClinSUM training set. Experimental results demonstrate that MedCOD significantly improves summarization quality, especially in non-English languages. Ablation studies show that both prompt-level augmentation and fine-tuning contribute to the observed performance gains. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chain of Dictionary; Contextual Augmentation; Medical Text Summarization; Multilingual NLP},
	keywords = {Chains; Computational linguistics; Natural language processing systems; Text processing; Back translations; Chain of dictionary; Contextual augmentation; Equivalence checking; Keyword-based; Medical text summarization; Multilingual NLP; Multilingual summarization; Semantic equivalences; Text Summarisation; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Santos202524,
	author = {Santos, Daniel and Nogueira, Vítor Beires and Quaresma, Paulo},
	title = {Parameter Efficient Fine-Tunning of LLMs: Application to Machine Translation from English to Portuguese},
	year = {2025},
	pages = {24 - 28},
	doi = {10.1109/ICCTech66294.2025.00014},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012573253&doi=10.1109%2FICCTech66294.2025.00014&partnerID=40&md5=3616e082edf583e26897332325677d1a},
	abstract = {Fine-tuning Large Language Models (LLMs) for specific tasks, such as machine translation, is a computationally expensive process that often requires substantial hardware resources. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) and Quantized Low-Rank Adaptation (QLoRA), offer a resource-efficient alternative by significantly reducing the number of trainable parameters and memory requirements. In this work, we compare the performance and memory efficiency of LoRA and QLoRA on English-Portuguese translation tasks, utilizing two cutting edge LLMs, Meta LLaMA 3.1 8B and Mistral 7B. Our experiments demonstrate that both LoRA and QLoRA achieve substantial memory savings. Moreover, this work underscores the practical advantages of LoRA and QLoRA in resource-constrained environments, providing a foundation for further optimization and experimentation in machine translation using large language models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {fine-tunning; LLM; LoRA; Machine Translation; QLoRA},
	keywords = {Artificial intelligence; Computational linguistics; Engineering research; Machine translation; Fine tuning; Fine-tunning; Hardware resources; Language model; Large language model; Low-rank adaptation; Machine translations; Model application; Quantized low-rank adaptation; Specific tasks; Constrained optimization},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zou2025,
	author = {Zou, Shuai and Liang, Xuefeng and Huang, Yiyang},
	title = {LipReading for Low-resource Languages by Language Dynamic LoRA},
	year = {2025},
	journal = {Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
	pages = {},
	doi = {10.1109/ICASSP49660.2025.10889645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009590894&doi=10.1109%2FICASSP49660.2025.10889645&partnerID=40&md5=5bfd28c8245fd7673055b10e21fb4960},
	abstract = {Lipreading in low-resource languages is a highly demanded yet extremely challenging task. Recent research has shown that transferring the lipreading knowledge from data-rich languages to low-resource ones can improve the performance. However, unique lip movements in low-resource languages and heterogeneous syntactic structures and lexical conventions across languages render such knowledge transfer ineffective. To address these challenges, we introduce a Dynamic Low-Rank Fine-tuning to adaptively adjust model parameters for learning a set of basic lip shapes shared across languages, thereby better recognizing lip movements, especially those unique to low-resource languages. Moreover, we design an instruction tuning on multilingual LLMs to enhance the model’s cross-lingual text mapping capability. Experiments conducted on low-resource language datasets, Italian and Portuguese, demonstrate the effectiveness of our proposed method. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Instruction tuning; Large language model; Lipreading; Low-Rank Fine-tuning; Low-resource languages},
	keywords = {Computational linguistics; Information systems; Information use; Knowledge transfer; Natural language processing systems; Syntactics; Tuning; Fine tuning; Instruction tuning; Language model; Large language model; Lip movements; Lipreading; Low resource languages; Low-rank fine-tuning; Performance; Recent researches; Knowledge management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Neto202419353,
	author = {Neto, José Carlos Ferreira and Pereira, Denilson Alves and Barbosa, Bruno Henrique Groenner and Ferreira, D. D.},
	title = {Approaches based on language models for aspect extraction for sentiment analysis in the Portuguese language},
	year = {2024},
	journal = {Neural Computing and Applications},
	volume = {36},
	number = {31},
	pages = {19353 - 19363},
	doi = {10.1007/s00521-024-10265-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200328401&doi=10.1007%2Fs00521-024-10265-4&partnerID=40&md5=a74771d1d68b6ada8c6d3e6ad7bc3122},
	abstract = {This work addresses the gap in aspect extraction techniques for Portuguese by adapting methods originally designed for English. It focuses on TV devices and literary reviews in the TV and ReLi datasets. For this, models based on the BERT architecture were employed, including pre-trained general domain (BERTimbau) and specific domain models (BERTtv and BERTreli). Also, this paper contributes with a novel double embedding technique that merges these models. We further explored the potential of large language models (LLMs) with a Portuguese-trained LLaMa variant, Cabrita. Efficient fine-tuning techniques such as LoRA (low-rank adaptation) for BERTimbau and QLoRA (quantized low-rank adaptation) for Cabrita were applied to optimize resource demands. The BERTimbau model, adjusted with LoRA, achieved the highest F1 scores (0.846 for TV and 0.615 for ReLi), while Cabrita showed lower performance (0.68 for TV and 0.46 for ReLi). This study underscores the potential of adapting and optimizing existing techniques for aspect extraction in Portuguese, marking a significant advancement in the field © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Aspect extraction; BERT; Language models; Natural language processing},
	keywords = {Computational linguistics; Sentiment analysis; Aspect extraction; BERT; Extraction techniques; It focus; Language model; Language processing; Natural language processing; Natural languages; Portuguese languages; Extraction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Vieira2024236,
	author = {Vieira, Inacio and Allred, Will and Lankford, Séamus and Castilho, Sheila and Way, Andy},
	title = {How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes},
	year = {2024},
	volume = {1},
	pages = {236 - 249},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217016565&partnerID=40&md5=94b483481f0673e200a994d285f907b4},
	abstract = {Decoder-only LLMs have shown impressive performance in MT due to their ability to learn from extensive datasets and generate high-quality translations. However, LLMs often struggle with the nuances and style required for organisation-specific translation. In this study, we explore the effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3 8B Instruct, leveraging translation memories (TMs), as a valuable resource to enhance accuracy and efficiency. We investigate the impact of fine-tuning the Llama 3 model using TMs from an organisation in the software sector. Our experiments cover five translation directions across languages of varying resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and Korean). We analyse diverse sizes of training datasets (1k to 207k segments) to evaluate their influence on translation quality. We fine-tune separate models for each training set and evaluate their performance based on automatic metrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in translation performance with larger datasets across all metrics. On average, BLEU and COMET scores increase by 13 and 25 points, respectively, on the largest training set against the baseline model. Notably, there is a performance deterioration in comparison with the baseline model when fine-tuning on only 1k and 2k examples; however, we observe a substantial improvement as the training dataset size increases. The study highlights the potential of integrating TMs with LLMs to create bespoke translation models tailored to the specific needs of businesses, thus enhancing translation quality and reducing turn-around times. This approach offers a valuable insight for organisations seeking to leverage TMs and LLMs for optimal translation outcomes, especially in narrower domains. © 2025 Elsevier B.V., All rights reserved.},
	keywords = {Program translators; Baseline models; Data set size; Fine tuning; Language model; Performance; Performances evaluation; Training dataset; Training sets; Translation memory; Translation quality; Computer aided language translation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Cabral2024167,
	author = {Cabral, Bruno Souza and Souza, Marlo Vieira and Claro, Daniela Barreiro},
	title = {Open Information Extraction with LLM for the Portuguese Language; Extração de Informação Aberta com LLM para a Língua Portuguesa},
	year = {2024},
	journal = {Linguamatica},
	volume = {16},
	number = {2},
	pages = {167 - 182},
	doi = {10.21814/lm.16.2.454},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216983339&doi=10.21814%2Flm.16.2.454&partnerID=40&md5=975a9feec2a63e8b969733ca311e2e81},
	abstract = {In this study, we investigate the application of Large Language Models (LLMs) for Open Information Extraction (OpenIE) in the Portuguese language. While most OpenIE methods have been developed with a focus on the English language, few works in the literature explore multilingual and cross-linguistic scenarios. Although there is a growing interest in OpenIE methods for Portuguese, the use of LLMs specifically focused on OpenIE in this language remains underexplored. We analyze the feasibility of incorporating both open and commercial LLMs using few-shot prompt engineering for OpenIE in Portuguese. We provide a detailed analysis of the performance of these LLMs in OpenIE tasks, demonstrating that they achieve performance metrics comparable to state-of-the-art systems. Additionally, we refine and release an open LLM for OpenIE, named PortOIE-Llama, which outperforms commercial LLMs in our experiments. Our results highlight the potential of LLMs in OpenIE tasks in Portuguese and suggest that further refinement and fine-tuning of larger models can further enhance these outcomes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {corpus; information extraction; LLM; OpenIE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Di Oliveira2024234,
	author = {Di Oliveira, Vinicius and Façanha Bezerra, Yuri and Li, Weigang and Carvalho Brom, Pedro and Celestino, Victor Rafael R.},
	title = {SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature},
	year = {2024},
	journal = {International Conference on Web Information Systems and Technologies, WEBIST - Proceedings},
	pages = {234 - 241},
	doi = {10.5220/0012943400003825},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216176929&doi=10.5220%2F0012943400003825&partnerID=40&md5=35895719837d68087f342ab888dd9dcf},
	abstract = {Natural language processing (NLP) has seen significant advancements with the advent of large language models (LLMs). However, substantial improvements are still needed for languages other than English, especially for specific domains like the applications of Mercosur Common Nomenclature (NCM), a Brazilian Harmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a foundational Portuguese LLM, as an LLM source to implement the NCM application processing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT) technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs. This approach retains the chain-of-thought (CoT) methodology for prompt development in a more concise and streamlined manner, utilizing brief and focused documents for training. The proposed model demonstrates an efficient and cost-effective alternative for fine-tuning smaller LLMs, significantly outperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the research focuses on NCM applications, the methodology can be easily adapted for HS applications worldwide. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-Tuning; HS; Large Language Model; NCM; Portuguese Language; Retrieval Augmented Generation},
	keywords = {Linguistics; Modeling languages; Natural language processing systems; Search engines; Fine tuning; Harmonized system; Language model; Language processing; Large language model; Natural languages; NCM; Performance; Portuguese languages; Retrieval augmented generation; Terminology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Akter20244156,
	author = {Akter, Syeda Sabrina and Anastasopoulos, Antonios},
	title = {A Study on Scaling Up Multilingual News Framing Analysis},
	year = {2024},
	pages = {4156 - 4173},
	doi = {10.18653/v1/2024.findings-naacl.260},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197882005&doi=10.18653%2Fv1%2F2024.findings-naacl.260&partnerID=40&md5=ebdf3041325fd123a961be08a1e1f70a},
	abstract = {Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains. Additionally, we show that a system trained on our crowdsourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (LLMs) for this task, finding that task-specific fine-tuning is a better approach than employing bigger non-specialized models. © 2025 Elsevier B.V., All rights reserved.},
	keywords = {Computational linguistics; Social aspects; Automatic translation; Bengalis; Language model; Multilingual context; Percentage points; Performance; Political issues; Public opinions; Scaling-up; Training corpus; Crowdsourcing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {20th European, Mediterranean, and Middle Eastern Conference, EMCIS 2023},
	year = {2024},
	journal = {Lecture Notes in Business Information Processing},
	volume = {502 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190427984&partnerID=40&md5=9a05ad2c4f010703f863c4665ee0b9cf},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on European, Mediterranean, and Middle Eastern. The topics include: Web Mining for Estimating Regulatory Blockchain Readiness; reviewing the Role of Secret Sharing Schemes in Electronic Payment Protocols; Decentralization of DAOs: A Fundamental Analysis; Blockchain-Powered NFTs: A Paradigm Shift in Carbon Credit Transactions for Traceability, Transparency, and Accountability; a Blockchain Framework for Digital Asset Ownership and Transfer in Succession; perspectives of Merchants Regarding Bitcoin’s Role as a Currency and Its Utility as a Payment System; a Chatbot Generator for Improved Digital Governance; A Structured Analysis of Domain-Specific Linked Open Vocabularies (LOV): Indicators for Interoperability and Reusability; predicting Digital Winners and Losers in Economic Crises Using Artificial Intelligence and Open Government Data; chatbot Technology Assessment: 40 Cases from Greece; the Effects of Economic Crisis on the Digitalization of the Greek Social Security; design, Implementation, and Evaluation of a Food Price Monitoring Tool for Supporting Data Journalists; Smartphone Apps for Parents of Preterm Infants from NICU to Home: A Quality, Evidence-Based Content and Data Protection Assessment; assessing the Progress of Portuguese Hospitals’ Online Services; Α Cross-Sector Data Space for Correlating Environmental Risks with Human Health; using Computational Knowledge Extraction Approach to Assess Three Decades of Health Management Information Systems for Informed Actions; the Role of Artificial Ethics Principles in Managing Knowledge and Enabling Data-Driven Decision Making in Supply Chain Management; fine-Tuning Large-Scale Project Scheduling; Integrating LLMs in Higher Education, Through Interactive Problem Solving and Tutoring: Algorithmic Approach and Use Cases. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {20th European, Mediterranean, and Middle Eastern Conference, EMCIS 2023},
	year = {2024},
	journal = {Lecture Notes in Business Information Processing},
	volume = {501 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190379632&partnerID=40&md5=79a8647de57760acd38abd48a24530f3},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on European, Mediterranean, and Middle Eastern. The topics include: Web Mining for Estimating Regulatory Blockchain Readiness; reviewing the Role of Secret Sharing Schemes in Electronic Payment Protocols; Decentralization of DAOs: A Fundamental Analysis; Blockchain-Powered NFTs: A Paradigm Shift in Carbon Credit Transactions for Traceability, Transparency, and Accountability; a Blockchain Framework for Digital Asset Ownership and Transfer in Succession; perspectives of Merchants Regarding Bitcoin’s Role as a Currency and Its Utility as a Payment System; a Chatbot Generator for Improved Digital Governance; A Structured Analysis of Domain-Specific Linked Open Vocabularies (LOV): Indicators for Interoperability and Reusability; predicting Digital Winners and Losers in Economic Crises Using Artificial Intelligence and Open Government Data; chatbot Technology Assessment: 40 Cases from Greece; the Effects of Economic Crisis on the Digitalization of the Greek Social Security; design, Implementation, and Evaluation of a Food Price Monitoring Tool for Supporting Data Journalists; Smartphone Apps for Parents of Preterm Infants from NICU to Home: A Quality, Evidence-Based Content and Data Protection Assessment; assessing the Progress of Portuguese Hospitals’ Online Services; Α Cross-Sector Data Space for Correlating Environmental Risks with Human Health; using Computational Knowledge Extraction Approach to Assess Three Decades of Health Management Information Systems for Informed Actions; the Role of Artificial Ethics Principles in Managing Knowledge and Enabling Data-Driven Decision Making in Supply Chain Management; fine-Tuning Large-Scale Project Scheduling; Integrating LLMs in Higher Education, Through Interactive Problem Solving and Tutoring: Algorithmic Approach and Use Cases. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{North2023404,
	author = {North, Kai and Dmonte, Alphaeus and Ranasinghe, Tharindu and Shardlow, Matthew and Zampieri, Marcos},
	title = {ALEXSIS+: Improving Substitute Generation and Selection for Lexical Simplification with Information Retrieval},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {404 - 413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174517237&partnerID=40&md5=b4d0f0e3751d2a1a8a430cb05293ea7f},
	abstract = {Lexical simplification (LS) automatically replaces words that are deemed difficult to understand for a given target population with simpler alternatives, whilst preserving the meaning of the original sentence. The TSAR-2022 shared task on LS provided participants with a multilingual lexical simplification test set. It contained nearly 1,200 complex words in English, Portuguese, and Spanish and presented multiple candidate substitutions for each complex word. The competition did not make training data available; therefore, teams had to use either off-the-shelf pre-trained large language models (LLMs) or out-domain data to develop their LS systems. As such, participants were unable to fully explore the capabilities of LLMs by re-training and/or fine-tuning them on in-domain data. To address this important limitation, we present ALEXSIS+, a multilingual dataset in the aforementioned three languages, and ALEXSIS++, an English monolingual dataset that together contains more than 50,000 unique sentences retrieved from news corpora and annotated with cosine similarities to the original complex word and sentence. Using these additional contexts, we are able to generate new high-quality candidate substitutions that improve LS performance on the TSAR-2022 test set regardless of the language or model. © 2023 Elsevier B.V., All rights reserved.},
	keywords = {Cosine similarity; Fine tuning; High quality; Language model; New high; News corpora; Performance; Simple++; Test sets; Training data; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Mehta2023453,
	author = {Mehta, Rahul and Varma, Vasudeva},
	title = {LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa},
	year = {2023},
	pages = {453 - 456},
	doi = {10.18653/v1/2023.semeval-1.62},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162822630&doi=10.18653%2Fv1%2F2023.semeval-1.62&partnerID=40&md5=d270ca23a1da25661bec44025a6dcb0b},
	abstract = {Named Entity Recognition(NER) is a task of recognizing entities at a token level in a sentence. This paper focuses on solving NER tasks in a multilingual setting for complex named entities.Our team, LLM-RM participated in the recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual Complex Named Entity Recognition. We approach the problem by leveraging cross-lingual representation provided by fine-tuning XLM-Roberta base model on datasets of all of the 12 languages provided - Bangla, Chinese, English, Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and Ukrainian. © 2025 Elsevier B.V., All rights reserved.},
	keywords = {Base models; Cross-lingual; Fine tuning; Named entities; Named entity recognition; Swedishs},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access}
}

@ARTICLE{2021,
	title = {26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12801 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111418311&partnerID=40&md5=8bab28af06067f417fc25b45578598c7},
	abstract = {The proceedings contain 33 papers. The special focus in this conference is on Applications of Natural Language to Information Systems. The topics include: The Importance of Character-Level Information in an Event Detection Model; sequence-Based Word Embeddings for Effective Text Classification; BERT-Capsule Model for Cyberbullying Detection in Code-Mixed Indian Languages; multiword Expression Features for Automatic Hate Speech Detection; semantic Text Segment Classification of Structured Technical Content; on the Generalization of Figurative Language Detection: The Case of Irony and Sarcasm; extracting Facts from Case Rulings Through Paragraph Segmentation of Judicial Decisions; Detection of Misinformation About COVID-19 in Brazilian Portuguese WhatsApp Messages; multi-Step Transfer Learning for Sentiment Analysis; scaling Federated Learning for Fine-Tuning of Large Language Models; improving Sentiment Classification in Low-Resource Bengali Language Utilizing Cross-Lingual Self-supervised Learning; human Language Comprehension in Aspect Phrase Extraction with Importance Weighting; exploring Summarization to Enhance Headline Stance Detection; predicting Vaccine Hesitancy and Vaccine Sentiment Using Topic Modeling and Evolutionary Optimization; sentiment Progression Based Searching and Indexing of Literary Textual Artefacts; argument Mining in Tweets: Comparing Crowd and Expert Annotations for Automated Claim and Evidence Detection; authorship Attribution Using Capsule-Based Fusion Approach; on the Explainability of Automatic Predictions of Mental Disorders from Social Media Data; using Document Embeddings for Background Linking of News Articles; let’s Summarize Scientific Documents! A Clustering-Based Approach via Citation Context; overcoming the Knowledge Bottleneck Using Lifelong Learning by Social Agents; cross-Active Connection for Image-Text Multimodal Feature Fusion; profiling Fake News Spreaders: Personality and Visual Information Matter; comparing MultiLingual and Multiple MonoLingual Models for Intent Classification and Slot Filling. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

